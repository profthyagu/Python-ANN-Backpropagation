{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Problem : Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below is a small contrived dataset that we can use to test out training our neural network.\n",
    "    X1\t\t\t     X2\t\t\tY\n",
    "2.7810836\t\t2.550537003\t\t0\n",
    "1.465489372\t\t2.362125076\t\t0\n",
    "3.396561688\t\t4.400293529\t\t0\n",
    "1.38807019\t\t1.850220317\t\t0\n",
    "3.06407232\t\t3.005305973\t\t0\n",
    "7.627531214\t\t2.759262235\t\t1\n",
    "5.332441248\t\t2.088626775\t\t1\n",
    "6.922596716\t\t1.77106367\t\t1\n",
    "8.675418651\t\t-0.242068655\t1\n",
    "7.673756466\t\t3.508563011\t\t1\n",
    "\n",
    "Below is the complete example. We will use 2 neurons in the hidden layer. It is a binary classification problem (2 classes) so there will be two neurons in the output layer. The network will be trained for 20 epochs with a learning rate of 0.5, which is high because we are training for so few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The input Data Set :\n",
      " [[2.7810836, 2.550537003, 0], [1.465489372, 2.362125076, 0], [3.396561688, 4.400293529, 0], [1.38807019, 1.850220317, 0], [3.06407232, 3.005305973, 0], [7.627531214, 2.759262235, 1], [5.332441248, 2.088626775, 1], [6.922596716, 1.77106367, 1], [8.675418651, -0.242068655, 1], [7.673756466, 3.508563011, 1]]\n",
      "\n",
      " Number of Inputs :\n",
      " 2\n",
      "\n",
      " Number of Outputs :\n",
      " 2\n",
      "\n",
      " The initialised Neural Network:\n",
      "\n",
      "\n",
      " Layer[1] Node[1]:\n",
      " {'weights': [0.4560342718892494, 0.4478274870593494, -0.4434486322731913]}\n",
      "\n",
      " Layer[1] Node[2]:\n",
      " {'weights': [-0.41512800484107837, 0.33549887812944956, 0.2359699890685233]}\n",
      "\n",
      " Layer[2] Node[1]:\n",
      " {'weights': [0.1697304014402209, -0.1918635424108558, 0.10594416567846243]}\n",
      "\n",
      " Layer[2] Node[2]:\n",
      " {'weights': [0.10680173364083789, 0.08120401711200309, -0.3416171297451944]}\n",
      "\n",
      " Network Training Begins:\n",
      "\n",
      ">epoch=0, lrate=0.500, error=5.278\n",
      ">epoch=1, lrate=0.500, error=5.122\n",
      ">epoch=2, lrate=0.500, error=5.006\n",
      ">epoch=3, lrate=0.500, error=4.875\n",
      ">epoch=4, lrate=0.500, error=4.700\n",
      ">epoch=5, lrate=0.500, error=4.466\n",
      ">epoch=6, lrate=0.500, error=4.176\n",
      ">epoch=7, lrate=0.500, error=3.838\n",
      ">epoch=8, lrate=0.500, error=3.469\n",
      ">epoch=9, lrate=0.500, error=3.089\n",
      ">epoch=10, lrate=0.500, error=2.716\n",
      ">epoch=11, lrate=0.500, error=2.367\n",
      ">epoch=12, lrate=0.500, error=2.054\n",
      ">epoch=13, lrate=0.500, error=1.780\n",
      ">epoch=14, lrate=0.500, error=1.546\n",
      ">epoch=15, lrate=0.500, error=1.349\n",
      ">epoch=16, lrate=0.500, error=1.184\n",
      ">epoch=17, lrate=0.500, error=1.045\n",
      ">epoch=18, lrate=0.500, error=0.929\n",
      ">epoch=19, lrate=0.500, error=0.831\n",
      "\n",
      " Network Training Ends:\n",
      "\n",
      "\n",
      " Final Neural Network :\n",
      "\n",
      " Layer[1] Node[1]:\n",
      " {'weights': [0.8642508164347665, -0.8497601716670763, -0.8668929014392035], 'output': 0.9295587965836384, 'delta': 0.005645382825629247}\n",
      "\n",
      " Layer[1] Node[2]:\n",
      " {'weights': [-1.2934302410111025, 1.7109363237151507, 0.7125327507327329], 'output': 0.04760703296164151, 'delta': -0.005928559978815076}\n",
      "\n",
      " Layer[2] Node[1]:\n",
      " {'weights': [-1.3098359335096292, 2.16462207144596, -0.3079052288835876], 'output': 0.19895563952058462, 'delta': -0.03170801648036037}\n",
      "\n",
      " Layer[2] Node[2]:\n",
      " {'weights': [1.5506793402414165, -2.11315950446121, 0.1333585709422027], 'output': 0.8095042653312078, 'delta': 0.029375796661413225}\n"
     ]
    }
   ],
   "source": [
    "# Author : Dr.Thyagaraju G S , Context Innovations Lab , DEpt of CSE , SDMIT - Ujire \n",
    "# Date : July 11 2018 \n",
    "import random\n",
    "from math import exp\n",
    "from random import seed\n",
    "\n",
    "# Initialize a network\n",
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random.uniform(-0.5,0.5) for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random.uniform(-0.5,0.5) for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    i= 1\n",
    "    print(\"\\n The initialised Neural Network:\\n\")\n",
    "    for layer in network:\n",
    "        j=1\n",
    "        for sub in layer:\n",
    "            print(\"\\n Layer[%d] Node[%d]:\\n\" %(i,j),sub)\n",
    "            j=j+1\n",
    "        i=i+1\n",
    "    return network\n",
    "\n",
    "# Calculate neuron activation (net) for an input\n",
    "\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    " \n",
    "# Transfer neuron activation to sigmoid function \n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        \n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        \n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    " \n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "   \n",
    "    print(\"\\n Network Training Begins:\\n\")\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    \n",
    "    print(\"\\n Network Training Ends:\\n\")\n",
    " \n",
    "\n",
    "#Test training backprop algorithm\n",
    "seed(2) \n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "\n",
    "print(\"\\n The input Data Set :\\n\",dataset)\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "print(\"\\n Number of Inputs :\\n\",n_inputs)\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "print(\"\\n Number of Outputs :\\n\",n_outputs)\n",
    "\n",
    "#Network Initialization\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "\n",
    "# Training the Network\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "\n",
    "\n",
    "print(\"\\n Final Neural Network :\")\n",
    "    \n",
    "i= 1\n",
    "for layer in network:\n",
    "    j=1\n",
    "    for sub in layer:\n",
    "        print(\"\\n Layer[%d] Node[%d]:\\n\" %(i,j),sub)\n",
    "        j=j+1\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with a trained neural network is easy enough.\n",
    "We have already seen how to forward-propagate an input pattern to get an output. This is all we need to do to make a prediction. We can use the output values themselves directly as the probability of a pattern belonging to each output class.\n",
    "It may be more useful to turn this output back into a crisp class prediction. We can do this by selecting the class value with the larger probability. This is also called the arg max function.\n",
    "Below is a function named predict() that implements this procedure. It returns the index in the network output that has the largest probability. It assumes that class values have been converted to integers starting at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Test making predictions with the network\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "#network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],\n",
    "#    [{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
